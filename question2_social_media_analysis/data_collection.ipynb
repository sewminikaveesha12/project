{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c090f1fd-3020-4721-a512-dba5daeca549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "  Found 20 books on page 1\n",
      "Scraping page 2...\n",
      "  Found 20 books on page 2\n",
      "Scraping page 3...\n",
      "  Found 20 books on page 3\n",
      "Scraping page 4...\n",
      "  Found 20 books on page 4\n",
      "Scraping page 5...\n",
      "  Found 20 books on page 5\n",
      "Scraping page 6...\n",
      "  Found 20 books on page 6\n",
      "Scraping page 7...\n",
      "  Found 20 books on page 7\n",
      "Scraping page 8...\n",
      "  Found 20 books on page 8\n",
      "Scraping page 9...\n",
      "  Found 20 books on page 9\n",
      "Scraping page 10...\n",
      "  Found 20 books on page 10\n",
      "Scraping page 11...\n",
      "  Found 20 books on page 11\n",
      "Scraping page 12...\n",
      "  Found 20 books on page 12\n",
      "Scraping page 13...\n",
      "  Found 20 books on page 13\n",
      "Scraping page 14...\n",
      "  Found 20 books on page 14\n",
      "Scraping page 15...\n",
      "  Found 20 books on page 15\n",
      "Scraping page 16...\n",
      "  Found 20 books on page 16\n",
      "Scraping page 17...\n",
      "  Found 20 books on page 17\n",
      "Scraping page 18...\n",
      "  Found 20 books on page 18\n",
      "Scraping page 19...\n",
      "  Found 20 books on page 19\n",
      "Scraping page 20...\n",
      "  Found 20 books on page 20\n",
      " Scraped 400 books from 20 pages\n",
      " Scraped 147 shop products\n",
      " Parsed 31 RSS items\n"
     ]
    }
   ],
   "source": [
    "# Data_collection\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# Safe HTTP request\n",
    "\n",
    "def safe_request(url, retries=3, delay=2):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Scrape books.toscrape.com \n",
    "\n",
    "BASE_URL = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "all_books = []\n",
    "\n",
    "\n",
    "for page_num in range(1, 21): \n",
    "    url = BASE_URL.format(page_num)\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    response = safe_request(url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        books = soup.select(\"article.product_pod\")\n",
    "        \n",
    "        for book in books:\n",
    "            title = book.h3.a[\"title\"]\n",
    "            price = book.select_one(\".price_color\").text.strip()\n",
    "            stock = book.select_one(\".availability\").text.strip()\n",
    "            rating = book.p[\"class\"][1]\n",
    "            \n",
    "            all_books.append({\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"stock\": stock,\n",
    "                \"rating\": rating\n",
    "            })\n",
    "        \n",
    "        print(f\"  Found {len(books)} books on page {page_num}\")\n",
    "        time.sleep(random.uniform(1, 3))  # polite delay between pages\n",
    "    else:\n",
    "        print(f\"  Failed to scrape page {page_num}\")\n",
    "\n",
    "print(f\" Scraped {len(all_books)} books from 20 pages\")\n",
    "\n",
    "\n",
    "# Save books to CSV + JSON\n",
    "if all_books:\n",
    "    with open(\"books.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=all_books[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_books)\n",
    "    \n",
    "    with open(\"books.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_books, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# Scrape e-commerce site\n",
    "\n",
    "shop_items = []\n",
    "base_url = \"https://webscraper.io\"\n",
    "\n",
    "response = safe_request(\"https://webscraper.io/test-sites/e-commerce/allinone\")\n",
    "if response:\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    # Get all category links\n",
    "    category_links = [base_url + a[\"href\"] for a in soup.select(\".category-link\")]\n",
    "    \n",
    "    for cat_link in category_links:\n",
    "        cat_resp = safe_request(cat_link)\n",
    "        if not cat_resp:\n",
    "            continue\n",
    "        cat_soup = BeautifulSoup(cat_resp.text, \"lxml\")\n",
    "        \n",
    "        category_name = cat_soup.select_one(\"h1\").text.strip() if cat_soup.select_one(\"h1\") else \"Unknown\"\n",
    "        \n",
    "        # Subcategory links\n",
    "        sub_links = [base_url + a[\"href\"] for a in cat_soup.select(\".subcategory-link\")]\n",
    "        \n",
    "        for sub_link in sub_links:\n",
    "            sub_resp = safe_request(sub_link)\n",
    "            if not sub_resp:\n",
    "                continue\n",
    "            sub_soup = BeautifulSoup(sub_resp.text, \"lxml\")\n",
    "            \n",
    "            subcategory_name = sub_soup.select_one(\"h1\").text.strip() if sub_soup.select_one(\"h1\") else \"Unknown\"\n",
    "            \n",
    "            products = sub_soup.select(\".thumbnail\")\n",
    "            for p in products:\n",
    "                name = p.select_one(\".title\")[\"title\"].strip()\n",
    "                price = p.select_one(\".price\").text.strip()\n",
    "                link = base_url + p.select_one(\".title\")[\"href\"]\n",
    "                \n",
    "                shop_items.append({\n",
    "                    \"name\": name,\n",
    "                    \"price\": price,\n",
    "                    \"link\": link,\n",
    "                    \"category\": category_name,\n",
    "                    \"subcategory\": subcategory_name\n",
    "                })\n",
    "            \n",
    "            time.sleep(random.uniform(1, 2))  # polite delay\n",
    "\n",
    "print(f\" Scraped {len(shop_items)} shop products\")\n",
    "\n",
    "\n",
    "# Save shop data\n",
    "if shop_items:\n",
    "    with open(\"shop.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=shop_items[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(shop_items)\n",
    "    \n",
    "    with open(\"shop.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(shop_items, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# Parse BBC RSS feed\n",
    "\n",
    "rss_items = []\n",
    "rss_url = \"http://feeds.bbci.co.uk/news/rss.xml\"\n",
    "response = safe_request(rss_url)\n",
    "\n",
    "if response:\n",
    "    root = ET.fromstring(response.content)\n",
    "    for item in root.findall(\".//item\"):\n",
    "        title = item.find(\"title\").text\n",
    "        link = item.find(\"link\").text\n",
    "        pub_date = item.find(\"pubDate\").text if item.find(\"pubDate\") is not None else \"N/A\"\n",
    "        rss_items.append({\"title\": title, \"link\": link, \"date\": pub_date})\n",
    "\n",
    "print(f\" Parsed {len(rss_items)} RSS items\")\n",
    "\n",
    "\n",
    "# Save RSS feed\n",
    "if rss_items:\n",
    "    with open(\"rss.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=rss_items[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rss_items)\n",
    "    \n",
    "    with open(\"rss.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rss_items, f, indent=4)                                                                                                           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86762df-60fb-4501-9718-a43f33043e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
