{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55082aa9-22fb-4b21-b95c-085278fa7038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books: (400, 5)\n",
      "Shop: (147, 4)\n",
      "RSS: (31, 3)\n"
     ]
    }
   ],
   "source": [
    "# Data_processing\n",
    "\n",
    "import requests, time, random, re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def safe_request(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        return requests.get(url, headers=headers, timeout=10)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_price(value):\n",
    "    value = re.sub(r\"[^\\d.]\", \"\", str(value))\n",
    "    return float(value) if value else None\n",
    "\n",
    "\n",
    "# Books\n",
    "\n",
    "books_data = []\n",
    "for page in range(1, 21):  \n",
    "    resp = safe_request(f\"http://books.toscrape.com/catalogue/page-{page}.html\")\n",
    "    if not resp: continue\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    for book in soup.select(\"article.product_pod\"):\n",
    "        title = book.h3.a[\"title\"]\n",
    "        price = clean_price(book.select_one(\".price_color\").text)\n",
    "        stock = book.select_one(\".availability\").text.strip()\n",
    "        rating = book.p[\"class\"][1]\n",
    "        # get category from detail page\n",
    "        link = \"http://books.toscrape.com/catalogue/\" + book.h3.a[\"href\"].replace(\"../\", \"\")\n",
    "        cat_resp = safe_request(link)\n",
    "        category = None\n",
    "        if cat_resp:\n",
    "            cat_soup = BeautifulSoup(cat_resp.text, \"lxml\")\n",
    "            bc = cat_soup.select(\"ul.breadcrumb li a\")\n",
    "            if len(bc) >= 3:\n",
    "                category = bc[2].text.strip()\n",
    "        books_data.append([title, price, stock, rating, category])\n",
    "    time.sleep(random.uniform(1,2))\n",
    "\n",
    "books_df = pd.DataFrame(books_data, columns=[\"title\",\"price\",\"stock\",\"rating\",\"category\"])\n",
    "\n",
    "\n",
    "# E-commerce site\n",
    "\n",
    "shop_data = []\n",
    "base_url = \"https://webscraper.io/test-sites/e-commerce/allinone\"\n",
    "resp = safe_request(base_url)\n",
    "if resp:\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    categories = soup.select(\"div.sidebar-nav ul li a.category-link\")\n",
    "    for cat in categories:\n",
    "        cat_name = cat.text.strip()\n",
    "        cat_url = \"https://webscraper.io\" + cat[\"href\"]\n",
    "        cat_resp = safe_request(cat_url)\n",
    "        if not cat_resp: continue\n",
    "        cat_soup = BeautifulSoup(cat_resp.text, \"lxml\")\n",
    "        subcats = cat_soup.select(\"div.sidebar-nav ul li ul li a.subcategory-link\")\n",
    "        for sub in subcats:\n",
    "            sub_name = sub.text.strip()\n",
    "            sub_url = \"https://webscraper.io\" + sub[\"href\"]\n",
    "            sub_resp = safe_request(sub_url)\n",
    "            if not sub_resp: continue\n",
    "            sub_soup = BeautifulSoup(sub_resp.text, \"lxml\")\n",
    "            for p in sub_soup.select(\".thumbnail\"):\n",
    "                name = p.select_one(\".title\").text.strip()\n",
    "                price = clean_price(p.select_one(\".price\").text)\n",
    "                shop_data.append([name, price, cat_name, sub_name])\n",
    "            time.sleep(random.uniform(1,2))\n",
    "\n",
    "shop_df = pd.DataFrame(shop_data, columns=[\"name\",\"price\",\"category\",\"subcategory\"])\n",
    "\n",
    "\n",
    "# RSS feed\n",
    "\n",
    "rss_data = []\n",
    "rss_url = \"http://feeds.bbci.co.uk/news/rss.xml\"\n",
    "resp = safe_request(rss_url)\n",
    "if resp:\n",
    "    root = ET.fromstring(resp.content)\n",
    "    for item in root.findall(\".//item\"):\n",
    "        rss_data.append([\n",
    "            item.find(\"title\").text,\n",
    "            item.find(\"link\").text,\n",
    "            item.find(\"pubDate\").text if item.find(\"pubDate\") is not None else None\n",
    "        ])\n",
    "\n",
    "rss_df = pd.DataFrame(rss_data, columns=[\"title\",\"link\",\"date\"])\n",
    "\n",
    "\n",
    "print(\"Books:\", books_df.shape)\n",
    "print(\"Shop:\", shop_df.shape)\n",
    "print(\"RSS:\", rss_df.shape)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b99d2-46ac-4f5b-959c-f9bdd3619259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
